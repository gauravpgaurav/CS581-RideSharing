{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create DB Conncection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import db_connect\n",
    "sql = db_connect.DatabaseConnect('CS581GROUP6','localhost','root','qwerty','3306')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import sys\n",
    "from geopy.distance import geodesic\n",
    "import requests\n",
    "import json\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_delay_percent = 0.20\n",
    "max_delay = 600\n",
    "max_walking_time_percent = 0.10\n",
    "max_walking_time = 240\n",
    "#AG - UIC\n",
    "#graphhoper_key_test = '5d92bf2d-b97c-47a4-a93f-0169d1d09a40'\n",
    "#GP - UIC\n",
    "#graphhoper_key_test = '69f69e94-6f53-4436-89c0-1d98b89c8707'\n",
    "#GP - ME\n",
    "#graphhoper_key_test = 'd407f98d-bfbe-475c-bf64-af45085528c5'\n",
    "#GP - ICLOUD\n",
    "graphhoper_key_test = 'bbf822ce-0fed-4827-a4c6-c697cc97c6cc'\n",
    "pool_duration = 5\n",
    "api_hits = 0\n",
    "destinationDict = {}\n",
    "\n",
    "filepath = 'RideSharingData/filtered_data_Jan2016.csv'\n",
    "#filepath = 'RideSharingData/filtered_data_2016-03-10.csv'\n",
    "#filepath = 'RideSharingData/filtered_data_2016-03-10_No-Walk.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GP - GMAIL\n",
    "#graphhoper_key_test = '7b0f35c6-6c30-4fb2-8572-23be956e770c'\n",
    "#AG - DATASET B\n",
    "#graphhoper_key_test = '01f52585-e7bd-46ec-bdfa-e2c118d6c08b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Mapping Coordinates to Destination ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdestid(lat,longt):\n",
    "    return df_destinations.iloc[df_destinations.apply(lambda x: np.linalg.norm(np.array([lat, longt]) - np.array([x['destLat'],x['destLong']])), axis=1).idxmin(axis = 0)]['destID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_map_dest_id(lat_start, lat_end, long_start, long_end, dest_lat, dest_long, num_of_iter, num_of_iter_limit):\n",
    "    \n",
    "    if lat_start + (lat_end - lat_start)/2 >= dest_lat:\n",
    "        lat_end = lat_start + (lat_end - lat_start)/2\n",
    "    else:\n",
    "        lat_start = lat_start + (lat_end - lat_start)/2\n",
    "\n",
    "    if long_start + (long_end - long_start)/2 >= dest_long:\n",
    "        long_end = long_start + (long_end - long_start)/2\n",
    "    else:\n",
    "        long_start = long_start + (long_end - long_start)/2\n",
    "    num_of_iter = num_of_iter + 1\n",
    "    if num_of_iter <= num_of_iter_limit:\n",
    "        return search_map_dest_id(lat_start, lat_end, long_start, long_end, dest_lat, dest_long, num_of_iter,num_of_iter_limit)\n",
    "\n",
    "    else:\n",
    "        filtered_dest = df_destinations.loc[(df_destinations['destLat'] >= lat_start) & (df_destinations['destLat'] <= lat_end) & (df_destinations['destLong'] >= long_start) & (df_destinations['destLong'] <= long_end)]\n",
    "        if len(filtered_dest) == 0:\n",
    "            return False\n",
    "        else:    \n",
    "            return df_destinations.iloc[filtered_dest.apply(lambda x: np.linalg.norm(np.array([dest_lat,dest_long]) - np.array([x['destLat'],x['destLong']])), axis=1).idxmin(axis = 0)]['destID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Pool Window Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPoolWindow(pickup_time, pool_start_time, pool_window_id):\n",
    "    \n",
    "    pool_end_time = pool_start_time + timedelta(minutes = pool_duration)\n",
    "    \n",
    "    if(pickup_time > pool_end_time):\n",
    "        \n",
    "        pool_window_id += 1\n",
    "    \n",
    "        while(pickup_time > pool_end_time):\n",
    "\n",
    "            pool_start_time += timedelta(minutes = pool_duration)\n",
    "            pool_end_time = pool_start_time + timedelta(minutes = pool_duration)\n",
    "                \n",
    "    return pool_start_time, pool_window_id\n",
    "\n",
    "def poolWindowAssignment():\n",
    "    \n",
    "    pool_window_id = 1 \n",
    "    pool_start_time = df['tpep_pickup_datetime'][0]\n",
    "    pool_window_id_list = []\n",
    "    req_pool_window_id_list = []\n",
    "    pool_start_time_list = []\n",
    "    pool_window_duration_list = []\n",
    "    \n",
    "    i = 0\n",
    "    pool_window_id_list.append(pool_window_id)\n",
    "    pool_start_time_list.append(pool_start_time)\n",
    "    pool_window_duration_list.append(pool_duration)\n",
    "    \n",
    "    for pickup_datetime in df.tpep_pickup_datetime:\n",
    "        \n",
    "        pool_start_time, pool_window_id = getPoolWindow(pickup_datetime, pool_start_time, pool_window_id)\n",
    "        req_pool_window_id_list.append(pool_window_id)\n",
    "\n",
    "        #print('pool_window_id_list[',i,'] ',pool_window_id_list[i],' pool_window_id: ',pool_window_id)\n",
    "        if(pool_window_id_list[i] != pool_window_id):\n",
    "            pool_window_id_list.append(pool_window_id)\n",
    "            pool_start_time_list.append(pool_start_time)\n",
    "            pool_window_duration_list.append(pool_duration)\n",
    "            i += 1\n",
    "    \n",
    "    df['pool_window_id'] = req_pool_window_id_list\n",
    "\n",
    "    pool_window_data = {'poolingWindowID': pool_window_id_list, 'poolingStartTime': pool_start_time_list, 'poolingDuration': pool_window_duration_list}\n",
    "    pool_window_df = pd.DataFrame(data=pool_window_data)\n",
    "    return pool_window_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Calculate Individual Ride Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRideTime(destID):\n",
    "    destObj = df_destinations.loc[(df_destinations['destID']==destID)]\n",
    "    #print(destObj['timeFromSrc'].values[0])\n",
    "    return destObj['timeFromSrc'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Calculate Individual Ride Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRideDistance(destID):\n",
    "    destObj = df_destinations.loc[(df_destinations['destID']==destID)]\n",
    "    #print(destObj['distFromSrc'].values[0])\n",
    "    return destObj['distFromSrc'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Calculate Delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDelay(tripTime):\n",
    "    #tripTime = (dropoffTime - pickupTime).seconds\n",
    "    trip_percent = tripTime * max_delay_percent\n",
    "    return min(trip_percent, max_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Calculate Walking Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWalkingTime(isWillingToWalk, tripTime):\n",
    "    if isWillingToWalk==1:\n",
    "        #tripTime = (dropoffTime - pickupTime).seconds\n",
    "        trip_percent = tripTime * max_walking_time_percent\n",
    "    elif isWillingToWalk==0:\n",
    "        return 0\n",
    "    return min(trip_percent, max_walking_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Calculate Cost for Ride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec_to_min(sec_time):\n",
    "    return sec_time/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meter_to_mile(meter_distance):\n",
    "    return meter_distance/1609.34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(isSharing, distance, time):\n",
    "    #Convert milliseconds to minutes\n",
    "    time = sec_to_min(time)\n",
    "    #distance = distance/1000\n",
    "    #Convert meters to miles\n",
    "    distance = meter_to_mile(distance)\n",
    "    if isSharing==True:\n",
    "        cost_per_mile = 1.00\n",
    "        cost_per_min = 0.18\n",
    "        base_fare = 0\n",
    "        booking_fee = 2.3\n",
    "        min_fare = 7.3\n",
    "    else:\n",
    "        cost_per_mile = 1.80\n",
    "        cost_per_min = 0.28\n",
    "        base_fare = 0\n",
    "        booking_fee = 2.3\n",
    "        min_fare = 7.3\n",
    "    #print(distance, time)\n",
    "    total_fare = (cost_per_mile * distance) + (cost_per_min * time) + base_fare + booking_fee + min_fare\n",
    "    return total_fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filepath)\n",
    "df_destinations = sql.query('select * from destination',True)\n",
    "\n",
    "df['tpep_pickup_datetime'] = [datetime.strptime(pickup_time, '%Y-%m-%d %H:%M:%S') \n",
    "                                          for pickup_time in df['tpep_pickup_datetime']]\n",
    "\n",
    "df['tpep_dropoff_datetime'] = [datetime.strptime(dropoff_time, '%Y-%m-%d %H:%M:%S') \n",
    "                                          for dropoff_time in df['tpep_dropoff_datetime']]\n",
    "import time\n",
    "start_time = time.time()\n",
    "df['destID'] = df.apply(lambda x: search_map_dest_id(40.69134374000000000000, 40.88140500000000000000, -74.04164664000000000000, -73.87790573000000000000, x['dropoff_latitude'], x['dropoff_longitude'], 0, 5),axis = 1)\n",
    "df.loc[df['destID']==False,'destID']=df.loc[df['destID']==False].apply(lambda x: getdestid(x['dropoff_latitude'], x['dropoff_longitude']),axis = 1)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['indvRideTime'] = df.apply(lambda x: getRideTime(x['destID']),axis = 1)\n",
    "df['indvRideDist'] = df.apply(lambda x: getRideDistance(x['destID']),axis = 1)\n",
    "df['indvRideCost'] = df.apply(lambda x: cost_function(False,x['trip_distance'],x['indvRideTime']),axis = 1)\n",
    "df['maxDelay'] = df.apply(lambda x: getDelay(x['indvRideTime']),axis = 1)\n",
    "df['maxWalkTime'] = df.apply(lambda x: getWalkingTime(x['isWillingToWalk'],x['indvRideTime']),axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DF_Request Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_cache_dictionary(df_cache):\n",
    "    for index, row in df_cache.iterrows():\n",
    "        key = (row['destID1'], row['destID2'])\n",
    "        value = (row['time1to2'], row['dist1to2'])\n",
    "        if key in destinationDict:\n",
    "            print('Exists', destinationDict[key])\n",
    "        else:\n",
    "            #print('Not Exists - Added')\n",
    "            destinationDict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_window_df = poolWindowAssignment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reqID'] =  df.index + 1\n",
    "request_df = df[['reqID', 'numberOfPassengers', 'isWillingToWalk', 'destID', 'maxDelay','maxWalkTime','tpep_pickup_datetime','indvRideTime','indvRideDist','indvRideCost','pool_window_id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_df=request_df.rename(index=str, columns={'tpep_pickup_datetime':'requestTime','trip_distance':'indvRideDist','pool_window_id':'poolingWindowID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinationDict ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cache = sql.query('select * from DESTINATION_CACHE',True)\n",
    "populate_cache_dictionary(df_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinationDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Insert into SQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pool_window_df\n",
    "sql.insert(pool_window_df,'POOLING_WINDOW');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql.insert(request_df,'REQUESTS');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Shareability Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRequestListForEachPoolId():\n",
    "    max_pool_window_id = df['pool_window_id'].max()\n",
    "    all_req_list = []\n",
    "    each_pool_req_list = []\n",
    "    \n",
    "    for x in range(1,  max_pool_window_id + 1):\n",
    "        each_pool_req_list = df.loc[df['pool_window_id'] == x]\n",
    "        all_req_list.append(each_pool_req_list)\n",
    "    return all_req_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharability_graph(pool_request_list):\n",
    "    \n",
    "    source_dest = (40.644190, -73.782366)\n",
    "    taxi_capacity = 4\n",
    "    graph = []\n",
    "    possible_match_shared_details_map = {}\n",
    "    all_req_set = set()\n",
    "    \n",
    "    for i in range(pool_request_list.shape[0]):\n",
    "    #for index, req_A in pool_request_list.iterrows():\n",
    "        req_A = pool_request_list.iloc[i,:]\n",
    "        req_id_A = req_A['reqID']\n",
    "        all_req_set.add(req_id_A)\n",
    "        dest_id_A = req_A['destID']\n",
    "        walk_time_A = req_A['maxWalkTime']\n",
    "        indv_ride_time_A = req_A['indvRideTime']\n",
    "        max_delay_A = req_A['maxDelay']\n",
    "        \n",
    "        for j in range(i+1, pool_request_list.shape[0]):\n",
    "        #for index+1, req_B in pool_request_list.iterrows():\n",
    "            \n",
    "            req_B = pool_request_list.iloc[j,:]\n",
    "            req_id_B = req_B['reqID']\n",
    "            all_req_set.add(req_id_B)\n",
    "            dest_id_B = req_B['destID']\n",
    "            indv_ride_time_B = req_B['indvRideTime']\n",
    "            max_delay_B = req_B['maxDelay']\n",
    "            walk_time_B = req_A['maxWalkTime']\n",
    "                \n",
    "            if req_id_A != req_id_B and req_A['passenger_count'] + req_B['passenger_count'] <= taxi_capacity:\n",
    "                \n",
    "                poss_dest_A_list = []\n",
    "                poss_dest_B_list = []\n",
    "                \n",
    "                if(apply_euclidean_elimination(req_A, req_B)==False):\n",
    "                    #print('Euc Fail')\n",
    "                    continue\n",
    "                \n",
    "                poss_dest_A_list , poss_dest_B_list = possibleDestinations(dest_id_A, walk_time_A, dest_id_B, walk_time_B)\n",
    "\n",
    "                benefit = 0\n",
    "                   \n",
    "                object_Dest1, object_Dest2, isReverse = select_route(source_dest, poss_dest_A_list, poss_dest_B_list)\n",
    "\n",
    "                shared_distance_1to2, shared_time_1to2 = compute_shared_distance_time(object_Dest1, object_Dest2)\n",
    "                \n",
    "                if isReverse:\n",
    "                    #B to A\n",
    "                    #Check A delay\n",
    "                    indv_ride_time_Dest2 = indv_ride_time_A\n",
    "                    max_delay_Dest2 = max_delay_A\n",
    "                    indv_ride_cost_Dest1 = req_B['indvRideCost']\n",
    "                    indv_ride_cost_Dest2 = req_A['indvRideCost']\n",
    "    \n",
    "                else:\n",
    "                    #A to B\n",
    "                    #Check B delay\n",
    "                    indv_ride_time_Dest2 = indv_ride_time_B\n",
    "                    max_delay_Dest2 = max_delay_B\n",
    "                    indv_ride_cost_Dest1 = req_A['indvRideCost']\n",
    "                    indv_ride_cost_Dest2 = req_B['indvRideCost']\n",
    "                  \n",
    "                indv_ride_time_srctoDest1 = object_Dest1['timeFromSrc'].values[0]\n",
    "                shared_distance_2 = object_Dest1['distFromSrc'].values[0] + shared_distance_1to2\n",
    "                shared_time_2 = object_Dest1['timeFromSrc'].values[0] + shared_time_1to2\n",
    "                \n",
    "                if shared_time_2 <= indv_ride_time_Dest2 + max_delay_Dest2:\n",
    "                    shared_ride_cost_Dest1 = cost_function(True, object_Dest1['distFromSrc'].values[0], object_Dest1['timeFromSrc'].values[0])\n",
    "                    shared_ride_cost_Dest2 = cost_function(True, shared_distance_2, shared_time_2)\n",
    "                    benefit = compute_benefit(indv_ride_cost_Dest1, indv_ride_cost_Dest2, shared_ride_cost_Dest1, shared_ride_cost_Dest2)\n",
    "                   \n",
    "                else:\n",
    "                    #print('Delay Fail')\n",
    "                    continue\n",
    "                    \n",
    "                graph.append((req_id_A, req_id_B, benefit))\n",
    "                if isReverse:\n",
    "                    #B to A\n",
    "                    req_id_1 = req_id_B\n",
    "                    req_id_2 = req_id_A\n",
    "                else:\n",
    "                    #A to B\n",
    "                    req_id_1 = req_id_A\n",
    "                    req_id_2 = req_id_B\n",
    "                possible_match_shared_details_map[str(req_id_1)+'&'+str(req_id_2)] = [object_Dest1['timeFromSrc'].values[0], object_Dest1['distFromSrc'].values[0], shared_ride_cost_Dest1, shared_time_2, shared_distance_2, shared_ride_cost_Dest2, req_A['pool_window_id'], object_Dest1['destID'].values[0], object_Dest2['destID'].values[0]]\n",
    "            #else:\n",
    "                #print('Cap Fail')\n",
    "    sorted_graph = sorted(graph, key=lambda x: x[2],  reverse=True)          \n",
    "    return sorted_graph, possible_match_shared_details_map, all_req_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possibleDestinations(id1, walk1, id2, walk2):\n",
    "    list1 = []\n",
    "    list1.append(id1)\n",
    "    list2 = []\n",
    "    list2.append(id2)\n",
    "\n",
    "    dest_tag = \"DEST\"\n",
    "    id1 = int(id1.split(dest_tag)[1])\n",
    "    id2 = int(id2.split(dest_tag)[1])\n",
    "\n",
    "    if(walk1 > 0):\n",
    "        list1 = createDestinationList(dest_tag,id1)\n",
    "\n",
    "    if(walk2 > 0):\n",
    "        list2 = createDestinationList(dest_tag,id2)\n",
    "        \n",
    "    if(walk1 > 120):\n",
    "        for i in range(0, len(list1)):\n",
    "            id0 = int(list1[i].split(dest_tag)[1])\n",
    "            list1.extend(createDestinationList(dest_tag,id0))\n",
    "\n",
    "    if(walk2 > 120):\n",
    "        for i in range(0, len(list2)):\n",
    "            id0 = int(list2[i].split(dest_tag)[1])\n",
    "            list2.extend(createDestinationList(dest_tag,id0))\n",
    "\n",
    "    return list(set(list1)),list(set(list2))\n",
    "\n",
    "def createDestinationList(dest_tag, dest_id_num):\n",
    "        list1 = []\n",
    "        list1.append(dest_tag+str(dest_id_num))\n",
    "        \n",
    "        #Corners\n",
    "        if(dest_id_num == 1):\n",
    "            list1.append(dest_tag+\"31\")\n",
    "            list1.append(dest_tag+\"2\")\n",
    "        elif(dest_id_num == 30):\n",
    "            list1.append(dest_tag+\"29\")\n",
    "            list1.append(dest_tag+\"60\")\n",
    "        elif(dest_id_num == 3331):\n",
    "            list1.append(dest_tag+\"3332\")\n",
    "            list1.append(dest_tag+\"3301\")\n",
    "        elif(dest_id_num == 3360):\n",
    "            list1.append(dest_tag+\"3359\")\n",
    "            list1.append(dest_tag+\"3330\")\n",
    "\n",
    "        #North edge\n",
    "        elif(dest_id_num <= 30):\n",
    "            list1.append(dest_tag+str(dest_id_num+1))\n",
    "            list1.append(dest_tag+str(dest_id_num-1))\n",
    "            list1.append(dest_tag+str(id+30))\n",
    "\n",
    "        #south edge\n",
    "        elif(dest_id_num >= 3330 and dest_id_num <= 3360):\n",
    "            list1.append(dest_tag+str(dest_id_num+1))\n",
    "            list1.append(dest_tag+str(dest_id_num-1))\n",
    "            list1.append(dest_tag+str(dest_id_num-30))\n",
    "\n",
    "        #west edge\n",
    "        elif(dest_id_num%30 == 1):\n",
    "            list1.append(dest_tag+str(dest_id_num+1))\n",
    "            list1.append(dest_tag+str(dest_id_num-30))\n",
    "            list1.append(dest_tag+str(dest_id_num+30))\n",
    "\n",
    "        #east edge\n",
    "        elif((dest_id_num%30) == 0):\n",
    "            list1.append(dest_tag+str(dest_id_num-1))\n",
    "            list1.append(dest_tag+str(dest_id_num+30))\n",
    "            list1.append(dest_tag+str(dest_id_num-30))\n",
    "        else:\n",
    "            list1.append(dest_tag+str(dest_id_num-1))\n",
    "            list1.append(dest_tag+str(dest_id_num+1))\n",
    "            list1.append(dest_tag+str(dest_id_num+30))\n",
    "            list1.append(dest_tag+str(dest_id_num-30))\n",
    "        return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_euclidean_elimination(reqA, reqB):\n",
    "    \n",
    "    #Retrieve destID for request A & B\n",
    "    destID_A = reqA['destID']\n",
    "    destID_B = reqB['destID']\n",
    "    \n",
    "    #Get coordinates for destID_A and destID_B\n",
    "    latA = df_destinations.loc[df_destinations['destID']==destID_A].iloc[0]['destLat']\n",
    "    lonA = df_destinations.loc[df_destinations['destID']==destID_A].iloc[0]['destLong']\n",
    "    latB = df_destinations.loc[df_destinations['destID']==destID_B].iloc[0]['destLat']\n",
    "    lonB = df_destinations.loc[df_destinations['destID']==destID_B].iloc[0]['destLong']\n",
    "    \n",
    "    #Calculate euclidean distance in meters\n",
    "    euc_distance = geodesic((latA,lonA), (latB,lonB)).km\n",
    "\n",
    "    #Using assumed average speed & euclidean distance calculate euclidean time from A to B\n",
    "    avg_speed = 32\n",
    "    calc_time_AtoB = (euc_distance/avg_speed)*3600\n",
    "    \n",
    "    #Check euclidean conditions for A to B and B to A\n",
    "    time_StoA = reqA['indvRideTime']\n",
    "    maxDelayA = reqA['maxDelay']\n",
    "    time_StoB = reqB['indvRideTime']\n",
    "    maxDelayB = reqA['maxDelay']\n",
    "    if (time_StoA + calc_time_AtoB < time_StoB + maxDelayB) and (time_StoB + calc_time_AtoB < time_StoA + maxDelayA):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will return the ID of first destination, ID of second destination, \n",
    "#distance from jfk to first destination, time from jfk to first destination\n",
    "def select_route(jfk, list1, list2):\n",
    "    #check all pairs\n",
    "    minDist = sys.maxsize\n",
    "    final_object_firstDest = df_destinations.loc[df_destinations['destID']==list1[0]]\n",
    "    final_object_secondDest = df_destinations.loc[df_destinations['destID']==list2[0]]\n",
    "    isReverse =  False\n",
    "\n",
    "    #check all pairs\n",
    "    for i in list1:\n",
    "        for j in list2:\n",
    "\n",
    "            aobject = df_destinations.loc[df_destinations['destID']==i]\n",
    "            bobject = df_destinations.loc[df_destinations['destID']==j]\n",
    "\n",
    "            a = (aobject[\"destLat\"].values[0], aobject[\"destLong\"].values[0])\n",
    "            b = (bobject[\"destLat\"].values[0], bobject[\"destLong\"].values[0])\n",
    "\n",
    "            jfkToA = aobject[\"distFromSrc\"].values[0]\n",
    "            jfkToB = bobject[\"distFromSrc\"].values[0]\n",
    "            # jfkToB = geodesic(jfk, b).miles\n",
    "            AToB = geodesic(a, b).km * 1000\n",
    "\n",
    "            totalAB = jfkToA+AToB\n",
    "            totalBA = jfkToB+AToB\n",
    "            # totalBA = jfkToB+AToB\n",
    "\n",
    "            #update indices if smaller route found, update minDist \n",
    "            if(totalAB < minDist):\n",
    "                minDist = totalAB\n",
    "                final_object_firstDest = aobject\n",
    "                final_object_secondDest = bobject\n",
    "                isReverse = False\n",
    "\n",
    "            if(totalBA < minDist):\n",
    "                minDist = totalBA\n",
    "                final_object_firstDest = bobject\n",
    "                final_object_secondDest = aobject\n",
    "                isReverse = True\n",
    "\n",
    "    #return indices in order\n",
    "    return final_object_firstDest, final_object_secondDest, isReverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkCache(aObject, bObject):\n",
    "    key = (aObject['destID'].values[0], bObject['destID'].values[0])\n",
    "    if key in destinationDict:\n",
    "        #print('FOUND IN CACHE', key)\n",
    "        value = destinationDict[key]\n",
    "        return value[0], value[1]\n",
    "    else:  \n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateCache(aObject, bObject, time, distance):\n",
    "    key = (aObject['destID'].values[0], bObject['destID'].values[0])\n",
    "    value = (time, distance)\n",
    "    destinationDict[key] = value\n",
    "    array = [key[0], key[1], value[0],value[1]]\n",
    "    df_cache= pd.DataFrame(np.array([array]), columns=['destID1', 'destID2', 'time1to2', 'dist1to2'])\n",
    "    #print('KEY')\n",
    "    #print(key[0], key[1])\n",
    "    sql.insert(df_cache,'DESTINATION_CACHE');\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return distance and time from a to b;\n",
    "#add this to min_jfk_to_a_dist and min_jfk_to_a_time return from the pervious method\n",
    "#to compute the total time and distance\n",
    "def compute_shared_distance_time(aObject, bObject):\n",
    "    #compute jfk to a, we have that in our dataframe\n",
    "    time, distance = checkCache(aObject, bObject)\n",
    "    if time is None and distance is None:\n",
    "        source = str(aObject[\"destLat\"].values[0])+ \", \" + str(aObject[\"destLong\"].values[0])\n",
    "        destination = str(bObject[\"destLat\"].values[0])+ \", \" + str(bObject[\"destLong\"].values[0])\n",
    "        URL = \"https://graphhopper.com/api/1/route?point=\" + source + \"&point=\" + destination + \"&vehicle=car&debug=true&key=\"+graphhoper_key_test+\"&type=json\"\n",
    "        r = requests.get(url = URL)\n",
    "        data = r.json()\n",
    "        #print(data)\n",
    "        global api_hits\n",
    "        api_hits+=1\n",
    "        #time in seconds\n",
    "        time = data['paths'][0]['time']/1000\n",
    "        distance = data['paths'][0]['distance']\n",
    "        updateCache(aObject, bObject, time, distance)\n",
    "    return distance, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_benefit(indv_ride_cost_Dest1, indv_ride_cost_Dest2, shared_ride_cost_Dest1, shared_ride_cost_Dest2):\n",
    "    return indv_ride_cost_Dest1+indv_ride_cost_Dest2-shared_ride_cost_Dest1-shared_ride_cost_Dest2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_match(sorted_graph):\n",
    "    g = nx.Graph()\n",
    "    g.add_weighted_edges_from(sorted_graph)\n",
    "    optimal_matches = nx.max_weight_matching(g, maxcardinality= False)\n",
    "    #fair_matches = nx.maximal_matching(g)\n",
    "    \n",
    "    return optimal_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_for_ride_table(optimal_matches, possible_match_shared_details_map, all_req_set, req_df, ride_id_counter):\n",
    "    \n",
    "    rides_table_records = []\n",
    "    for match in optimal_matches:\n",
    "        match_name = str(match[0])+'&'+str(match[1])\n",
    "        match_name_rev = str(match[1])+'&'+str(match[0])\n",
    "        req1_details = []\n",
    "        req2_details = []\n",
    "        if match_name in possible_match_shared_details_map:\n",
    "            match_shared_details = possible_match_shared_details_map[match_name]\n",
    "            req1_details = [match[0], ride_id_counter, 1, match_shared_details[7], match_shared_details[0], match_shared_details[1] , match_shared_details[2], match_shared_details[6]]\n",
    "            req2_details = [match[1], ride_id_counter, 1, match_shared_details[8], match_shared_details[3], match_shared_details[4] , match_shared_details[5], match_shared_details[6]]\n",
    "        elif match_name_rev in possible_match_shared_details_map:\n",
    "            match_shared_details = possible_match_shared_details_map[match_name_rev]\n",
    "            req1_details = [match[1], ride_id_counter, 1, match_shared_details[8], match_shared_details[3], match_shared_details[4] , match_shared_details[5], match_shared_details[6]]\n",
    "            req2_details = [match[0], ride_id_counter, 1, match_shared_details[7], match_shared_details[0], match_shared_details[1] , match_shared_details[2], match_shared_details[6]]\n",
    "        \n",
    "        if len(req1_details)> 0 and len(req2_details)>0:\n",
    "            rides_table_records.append(req1_details)\n",
    "            rides_table_records.append(req2_details)\n",
    "            all_req_set.remove(match[0])\n",
    "            all_req_set.remove(match[1])\n",
    "            ride_id_counter += 1\n",
    "            \n",
    "    for req_id in all_req_set:\n",
    "        actualDestID = req_df.loc[req_df['reqID'] == req_id, 'destID'].iloc[0]\n",
    "        indvRideTime = req_df.loc[req_df['reqID'] == req_id, 'indvRideTime'].iloc[0]\n",
    "        indvRideDist = req_df.loc[req_df['reqID'] == req_id, 'indvRideDist'].iloc[0]\n",
    "        indvRideCost = req_df.loc[req_df['reqID'] == req_id, 'indvRideCost'].iloc[0]\n",
    "        poolingWindowID = req_df.loc[req_df['reqID'] == req_id, 'poolingWindowID'].iloc[0]\n",
    "        req_details = [req_id, ride_id_counter, 0, actualDestID, indvRideTime, indvRideDist, indvRideCost, poolingWindowID]\n",
    "        rides_table_records.append(req_details)\n",
    "        ride_id_counter += 1\n",
    "    \n",
    "    rides_df = pd.DataFrame(np.array(rides_table_records), columns=['reqID', 'rideID', 'isSharing', 'actualDestID', 'sharedRideTime', 'sharedRideDist', 'sharedRideCost', 'poolingWindowID'])\n",
    "    \n",
    "    return rides_df, ride_id_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ride_id_counter = 1\n",
    "all_req_list = getRequestListForEachPoolId()\n",
    "graph =[]\n",
    "api_hits = 0\n",
    "rides_df = pd.DataFrame(columns=['reqID', 'rideID', 'isSharing', 'actualDestID', 'sharedRideTime', 'sharedRideDist', 'sharedRideCost', 'poolingWindowID'])\n",
    "print('Number of Pools :', str(len(all_req_list)))\n",
    "print('BEFORE API HITS', api_hits)\n",
    "total_time = 0\n",
    "isTimedOut = False\n",
    "\n",
    "for pool_index in range(len(all_req_list)):\n",
    "    pool_request_list = all_req_list[pool_index]\n",
    "    start_time = time.time()\n",
    "    sorted_graph, possible_match_shared_details_map, all_req_set = sharability_graph(pool_request_list)\n",
    "    optimal_matches = optimal_match(sorted_graph)\n",
    "    rides_df, ride_id_counter = create_data_for_ride_table(optimal_matches, possible_match_shared_details_map, all_req_set, request_df, ride_id_counter)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    total_time+=elapsed_time\n",
    "    if pool_request_list['pool_window_id'].iloc[0]%5 == 0:\n",
    "        print('Pool ID : ' + str(pool_request_list['pool_window_id'].iloc[0]) + '\\tPool Size : ' + str(len(pool_request_list)) + '\\t Time : ' + str(elapsed_time))\n",
    "    \n",
    "    if isTimedOut==False and total_time >300:\n",
    "        print('! Timed Out !\\t Total Time = ' + str(total_time))\n",
    "        print('Number of Pools processed = ' + str(pool_request_list['pool_window_id'].iloc[0]))\n",
    "        isTimedOut = True\n",
    "    \n",
    "    rides_df['reqID'] = rides_df['reqID'].astype('int')\n",
    "    sql.query(\"\"\"update pooling_window set poolingWindowSize= {}, poolingWindowComputeTime = {} where poolingWindowID = '{}'; \"\"\".format(len(pool_request_list), elapsed_time, str(pool_request_list['pool_window_id'].iloc[0])),df_flag=False)\n",
    "    sql.insert(rides_df,'RIDES');\n",
    "\n",
    "print('AFTER API HITS', api_hits)\n",
    "print('Total Time = ' + str(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
